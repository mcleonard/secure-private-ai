{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Learning Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a brief demo on using PySyft for federated learning. Check out [the main tutorials](https://github.com/OpenMined/PySyft/tree/dev/examples/tutorials) to learn more.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import syft as sy\n",
    "hook = sy.TorchHook(torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create some tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1,2,3,4,5])\n",
    "y = torch.tensor([1,1,1,1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a worker and send the tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = sy.VirtualWorker(hook, id=\"mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ptr = x.send(mat)\n",
    "y_ptr = y.send(mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates PointerTensors on our client that point to the tensors on the worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Wrapper)>[PointerTensor - 3507829112@mat]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{3507829112: tensor([1, 2, 3, 4, 5]), 9150654653: tensor([1, 1, 1, 1, 1])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x_ptr)\n",
    "mat._objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tensors are now on our worker. Operations on the PointerTensors (on our machine) are performed on the worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Wrapper)>[PointerTensor - 29706455715@mat]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{3507829112: tensor([1, 2, 3, 4, 5]),\n",
       " 9150654653: tensor([1, 1, 1, 1, 1]),\n",
       " 29706455715: tensor([ 2,  4,  6,  8, 10])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = x_ptr + x_ptr\n",
    "print(z)\n",
    "mat._objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get tensors back from the worker using `.get`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2,  4,  6,  8, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{3507829112: tensor([1, 2, 3, 4, 5]), 9150654653: tensor([1, 1, 1, 1, 1])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = z.get()\n",
    "print(z)\n",
    "mat._objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do **backpropagation** on the worker as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1,2,3,4,5.], requires_grad=True).send(mat)\n",
    "y = torch.tensor([1,1,1,1,1.], requires_grad=True).send(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = (x + y).sum()\n",
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 4., 5.], requires_grad=True)\n",
      "tensor([1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "x = x.get()\n",
    "print(x)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training models remotely\n",
    "\n",
    "Since we can do backprop on the worker, we should be able to train a model on the worker as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from syft import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new worker\n",
    "alice = sy.VirtualWorker(hook, id=\"alice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=2, out_features=1, bias=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A Toy Dataset\n",
    "data = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1.]])\n",
    "target = torch.tensor([[0], [0], [1], [1.]])\n",
    "\n",
    "# Iniitalize A Toy Model\n",
    "model = nn.Linear(2, 1)\n",
    "\n",
    "# send data to alice\n",
    "data_alice = data.send(alice)\n",
    "target_alice = target.send(alice)\n",
    "\n",
    "# send model to alice\n",
    "model.send(data_alice.location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{41907534508: tensor([[0., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 1.]]), 42445027957: tensor([[0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.]]), 98559516141: Parameter containing:\n",
      "tensor([[-0.3651, -0.1132]], requires_grad=True), 52943628406: Parameter containing:\n",
      "tensor([-0.1512], requires_grad=True)}\n"
     ]
    }
   ],
   "source": [
    "print(alice._objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, target, model):\n",
    "    # Training Logic\n",
    "    opt = optim.SGD(params=model.parameters(),lr=0.1)\n",
    "    for step in range(10):\n",
    "        # 1) erase previous gradients (if they exist)\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # 2) make a prediction\n",
    "        pred = model(data)\n",
    "\n",
    "        # 3) calculate how much we missed\n",
    "        loss = ((pred - target)**2).sum()\n",
    "\n",
    "        # 4) figure out which weights caused us to miss\n",
    "        loss.backward()\n",
    "\n",
    "        # 5) change those weights\n",
    "        opt.step(data.shape[0])\n",
    "\n",
    "        # 6) print our progress\n",
    "        print(loss.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.0467, requires_grad=True)\n",
      "tensor(2.8338, requires_grad=True)\n",
      "tensor(1.7639, requires_grad=True)\n",
      "tensor(1.2283, requires_grad=True)\n",
      "tensor(0.9444, requires_grad=True)\n",
      "tensor(0.7804, requires_grad=True)\n",
      "tensor(0.6751, requires_grad=True)\n",
      "tensor(0.5995, requires_grad=True)\n",
      "tensor(0.5401, requires_grad=True)\n",
      "tensor(0.4903, requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Linear(in_features=2, out_features=1, bias=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(data_alice, target_alice, model)\n",
    "# Get the model back after training\n",
    "model.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Update Averaging\n",
    "\n",
    "While this example is a nice introduction to Federated Learning, it still has some major shortcomings. Most notably, when we call `model.get()` and receive the updated model from Alice, we can actually learn a lot about Alice's training data by looking at their gradients. In some cases, we can restore their training data perfectly! \n",
    "\n",
    "So, what is there to do? Well, the first strategy people employ is to **average the gradient across multiple individuals before uploading it to the central server**. This strategy, however, will require some more sophisticated use of PointerTenor objects.\n",
    "\n",
    "I'll create a new worker, Bob, and use Mat as a trusted third party where I'll do the averaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new worker\n",
    "bob = sy.VirtualWorker(hook, id=\"bob\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to tell our workers about each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bob.add_workers([alice, mat])\n",
    "alice.add_workers([bob, mat])\n",
    "mat.add_workers([alice, bob])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can split up our data and send it to the workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get pointers to training data on each worker by\n",
    "# sending some training data to bob and alice\n",
    "bobs_data = data[0:2].send(bob)\n",
    "bobs_target = target[0:2].send(bob)\n",
    "\n",
    "alices_data = data[2:].send(alice)\n",
    "alices_target = target[2:].send(alice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model and send it to our two workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iniitalize A Toy Model\n",
    "model = nn.Linear(2,1)\n",
    "\n",
    "bobs_model = model.copy().send(bob)\n",
    "alices_model = model.copy().send(alice)\n",
    "\n",
    "bobs_opt = optim.SGD(params=bobs_model.parameters(), lr=0.1)\n",
    "alices_opt = optim.SGD(params=alices_model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train models on the two workers in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bob: tensor(0.1923)  Alice: tensor(1.2013)\n",
      "Bob: tensor(0.1084)  Alice: tensor(0.3670)\n",
      "Bob: tensor(0.0624)  Alice: tensor(0.1192)\n",
      "Bob: tensor(0.0371)  Alice: tensor(0.0450)\n",
      "Bob: tensor(0.0231)  Alice: tensor(0.0222)\n",
      "Bob: tensor(0.0152)  Alice: tensor(0.0147)\n",
      "Bob: tensor(0.0107)  Alice: tensor(0.0118)\n",
      "Bob: tensor(0.0081)  Alice: tensor(0.0103)\n",
      "Bob: tensor(0.0065)  Alice: tensor(0.0093)\n",
      "Bob: tensor(0.0055)  Alice: tensor(0.0084)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    # Train Bob's Model\n",
    "    bobs_opt.zero_grad()\n",
    "    bobs_pred = bobs_model(bobs_data)\n",
    "    bobs_loss = ((bobs_pred - bobs_target)**2).sum()\n",
    "    bobs_loss.backward()\n",
    "\n",
    "    bobs_opt.step(bobs_data.shape[0])\n",
    "    bobs_loss = bobs_loss.get().data\n",
    "\n",
    "    # Train Alice's Model\n",
    "    alices_opt.zero_grad()\n",
    "    alices_pred = alices_model(alices_data)\n",
    "    alices_loss = ((alices_pred - alices_target)**2).sum()\n",
    "    alices_loss.backward()\n",
    "\n",
    "    alices_opt.step(alices_data.shape[0])\n",
    "    alices_loss = alices_loss.get().data\n",
    "    \n",
    "    print(\"Bob: \" + str(bobs_loss) + \"  Alice: \" + str(alices_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll move the models to our trusted worker and average the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "alices_model.move(mat)\n",
    "bobs_model.move(mat)\n",
    "\n",
    "avg_weights = ((alices_model.weight.data + bobs_model.weight.data) / 2)\n",
    "avg_bias = ((alices_model.bias.data + bobs_model.bias.data) / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, move those averaged weights back to our centralized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3639])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weight.data.set_(avg_weights.get())\n",
    "model.bias.data.set_(avg_bias.get())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a non-demo setting you would do this whole training loop a bunch of times. Also, it's in general preferred to average the gradients rather than the models. Again, please visit [the main tutorials](https://github.com/OpenMined/PySyft/tree/dev/examples/tutorials) to learn more about federated learning with PySyft."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
